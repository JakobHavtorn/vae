{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T22:57:45.845254Z",
     "start_time": "2019-10-11T22:57:45.392801Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T22:57:45.854421Z",
     "start_time": "2019-10-11T22:57:45.847207Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "SEED = 1\n",
    "BATCH_SIZE = 64\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 10\n",
    "N_SAMPLES = 10\n",
    "LATENT_DIM = 20\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T22:57:45.866702Z",
     "start_time": "2019-10-11T22:57:45.859034Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T22:57:45.875793Z",
     "start_time": "2019-10-11T22:57:45.869181Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': NUM_WORKERS, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:43:12.423536Z",
     "start_time": "2019-10-11T23:43:12.378491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download or load downloaded MNIST dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    './',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        #transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0,), (255,)),\n",
    "    ])\n",
    "    \n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    './',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0,), (255,)),\n",
    "    ])\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:43:13.137000Z",
     "start_time": "2019-10-11T23:43:12.971601Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "plt.imshow(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:43:13.643691Z",
     "start_time": "2019-10-11T23:43:13.633760Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Convolutions with same padding\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(4, 4), padding=(15, 15), stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(4, 4), padding=(15, 15), stride=2)\n",
    "        self.fc_mu = nn.Linear(in_features=16 * 28 * 28, out_features=latent_dim)\n",
    "        self.fc_logvar = nn.Linear(in_features=16 * 28 * 28, out_features=latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(-1, 16 * 28 * 28)\n",
    "        mu_z = self.fc_mu(x)\n",
    "        logvar_z = self.fc_logvar(x)\n",
    "        return mu_z, logvar_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:43:14.171148Z",
     "start_time": "2019-10-11T23:43:14.157759Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Convolutions with same padding\n",
    "        self.fc1 = nn.Linear(in_features=latent_dim, out_features=128 * 7 * 7)\n",
    "        self.conv_t1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, padding=1, stride=2)\n",
    "        self.conv_t2 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, padding=1, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, z: Variable) -> Variable:\n",
    "        x = self.relu(self.fc1(z))\n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "        x = self.relu(self.conv_t1(x))\n",
    "        x = torch.sigmoid(self.conv_t2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:59:04.258141Z",
     "start_time": "2019-10-11T23:59:04.245147Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, n_samples, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.n_samples = n_samples\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        if self.training:\n",
    "            z_samples = []\n",
    "            for _ in range(self.n_samples):\n",
    "                # Multiply log variance with 0.5, then in-place exponent\n",
    "                # yielding the standard deviation\n",
    "                std = logvar.mul(0.5).exp()  # type: Variable\n",
    "                # eps = Variable(std.data.new(std.size()).normal_())\n",
    "                # Sample a noise term from a standard normal\n",
    "                eps = torch.randn_like(std)\n",
    "                z_samples.append(eps.mul(std).add_(mu))\n",
    "            return z_samples\n",
    "        # During inference, we simply spit out the mean of the\n",
    "        # learned distribution for the current input.  We could\n",
    "        # use a random sample from the distribution, but mu of\n",
    "        # course has the highest probability.\n",
    "        return mu\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        z = Variable(torch.randn(n_samples, self.latent_dim)).to(device)\n",
    "        return self.decoder.forward(z).cpu()\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encoder.forward(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.training:\n",
    "            return [self.decoder.forward(z) for z in z], mu, logvar\n",
    "        return self.decoder.forward(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:59:05.798771Z",
     "start_time": "2019-10-11T23:59:05.787981Z"
    }
   },
   "outputs": [],
   "source": [
    "class KLDivergenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, mu, logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.shape[0]\n",
    "\n",
    "\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, xhat_samples, x):\n",
    "        if self.training:\n",
    "            BCE = 0\n",
    "            for xhat in xhat_samples:\n",
    "                BCE += F.binary_cross_entropy(xhat.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "            return BCE / len(xhat_samples) / BATCH_SIZE\n",
    "        BCE = F.binary_cross_entropy(xhat_samples[0].view(-1, 784), x.view(-1, 784), reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:59:10.083170Z",
     "start_time": "2019-10-11T23:59:10.070459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "encoder = Encoder(LATENT_DIM)\n",
    "decoder = Decoder(LATENT_DIM)\n",
    "vae = VAE(encoder, decoder, N_SAMPLES, LATENT_DIM)\n",
    "vae.to(device)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "kl_divergence_loss = KLDivergenceLoss()\n",
    "recon_loss = ReconstructionLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:57:36.934668Z",
     "start_time": "2019-10-11T23:57:36.921056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    # in the case of MNIST, len(train_loader.dataset) is 60000\n",
    "    # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = Variable(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xhat, mu, logvar = vae.forward(x)\n",
    "        \n",
    "        rc_loss = recon_loss(xhat, x)\n",
    "        kl_loss =  kl_divergence_loss(mu, logvar)\n",
    "        loss = rc_loss + kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tRCLoss: {:.6f}, KLLoss: {:.6f}'.format(epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       rc_loss.item(), kl_loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "    \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "        data = Variable(data, volatile=True)\n",
    "        xhat, mu, logvar = vae(data)\n",
    "        \n",
    "        rc_loss = recon_loss(xhat, x)\n",
    "        kl_loss =  kl_divergence_loss(mu, logvar)\n",
    "        loss = rc_loss + kl_loss\n",
    "\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "            # with right below them the reconstructed output digits\n",
    "            comparison = torch.cat([data[:n], xhat.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(), './mnist/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:57:46.724976Z",
     "start_time": "2019-10-11T23:57:37.650073Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:59:48.062056Z",
     "start_time": "2019-10-11T23:59:47.911335Z"
    }
   },
   "outputs": [],
   "source": [
    "xgen = vae.sample()\n",
    "xgen = xgen.detach().numpy()[0, 0, ...]\n",
    "plt.imshow(xgen)\n",
    "xgen.max(), xgen.min(), xgen.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:45:22.521916Z",
     "start_time": "2019-10-11T23:45:22.341708Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(x.max())\n",
    "plt.imshow(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:45:23.033441Z",
     "start_time": "2019-10-11T23:45:23.028819Z"
    }
   },
   "outputs": [],
   "source": [
    "#encoder = Encoder(LATENT_DIM)\n",
    "#decoder = Decoder(LATENT_DIM)\n",
    "#vae = VAE(encoder, decoder, N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:45:23.541621Z",
     "start_time": "2019-10-11T23:45:23.532960Z"
    }
   },
   "outputs": [],
   "source": [
    "mu, logvar = encoder.forward(x[0, ...])\n",
    "mu, logvar.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:45:24.211424Z",
     "start_time": "2019-10-11T23:45:24.039086Z"
    }
   },
   "outputs": [],
   "source": [
    "xhat = decoder.forward(mu)\n",
    "plt.imshow(xhat.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T23:45:24.851413Z",
     "start_time": "2019-10-11T23:45:24.440866Z"
    }
   },
   "outputs": [],
   "source": [
    "xhat, mu, logvar = vae.forward(x)\n",
    "plt.imshow(xhat[0].detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vae] *",
   "language": "python",
   "name": "conda-env-vae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
